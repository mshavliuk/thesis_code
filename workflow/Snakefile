import os

from snakemake.io import (
    directory,
    expand,
)

ALL_PRETRAIN_EXPERIMENTS = glob_wildcards("experiments/pretrain/{file}.yaml").file

envvars:
    "DATA_DIR",
    "TEMP_DIR",
    "WANDB_PROJECT"

include: "rules/common.smk"

data_dir = os.environ["DATA_DIR"]

configfile: "workflow/config.yaml"


rule all:
    input:
        get_finetune_all_dependent_paths,


rule download_data:
    conda:
        "gcloud"
    output:
        temp(f"{data_dir}/raw/{{file}}.csv.gz"),
    log:
        "logs/download_data/{file}.log",
    shell:
        """
        gsutil -u $(gcloud config get-value project) cp \
        gs://mimiciii-1.4.physionet.org/{wildcards.file}.csv.gz {output} \
        > {log} 2> {log}
        """


rule convert_data_to_parquet:
    input:
        f"{data_dir}/raw/{{file}}.csv.gz",
    output:
        directory(f"{data_dir}/raw/{{file}}.parquet"),
    log:
        "logs/convert_to_parquet/{file}.log",
    threads: 2,
    resources:
        mem_mb=4096,
        runtime='1h',
    shell:
        """
        python workflow/scripts/csv_gz_to_parquet.py {input} {output} > {log} 2> {log}
        """

checkpoint generate_dataset:
    input:
        expand(
            f"{data_dir}/raw/{{mimic_file}}.parquet",
            mimic_file=config["mimic_files"],
        ),
    params:
        cli_args=lambda wc: dict_to_cli_args(config['datasets'][wc.dataset_name]),
    output:
        dataset=directory(f"{data_dir}/datasets/{{dataset_name}}"),
        status="results/generate_dataset/{dataset_name}.SUCCESS",
    log:
        "logs/generate_dataset/{dataset_name}.log",
    threads: min(workflow.cores,16),  # all available cores or 16, whichever is smaller
    resources:
        mem_mb=26624,# 26GB
        runtime='1h',
    shell:
        """
        python workflow/scripts/data_processing_job.py {params.cli_args} --output-path={output.dataset} > {log} 2> {log}
        
        if [ $? -eq 0 ]; then
            touch {output.status}
        fi
        """


rule generate_unittest_dataset:
    input:
        lambda wc: checkpoints.generate_dataset.get(dataset_name="original_strats_data").output,
    output:
        events="src/util/tests/data/events.parquet",
        labels="src/util/tests/data/labels.parquet",
        status="results/generate_dataset/unittest.SUCCESS",
    log:
        "logs/generate_dataset/unittest.log",
    params:
        data_dir=data_dir
    threads: 2,
    resources:
        mem_mb=2048,
        runtime='1h',
    shell:
        """
        python workflow/scripts/generate_unittest_dataset.py \
            -i {params.data_dir}/datasets/original_strats_data/test \
            > {log} 2> {log} \
            && touch {output.status}
        """

rule pretrain:
    input:
        config="experiments/pretrain/{file}.yaml",
        datasets=get_pretrain_dependency_paths,
    output:
        "results/pretrain/{file}_fold-{fold}.SUCCESS",
    log:
        "logs/pretrain/{file}_fold-{fold}.log",
    resources:
        mem_mb=8192,# 8GB
        runtime='4h',
        gpu=1,
    shell:
        """
        python src/pretrain.py --config {input.config} \
            > {log} 2> {log} \
            && touch {output}
        """


rule pretrain_all:
    input:
        get_pretrain_all_dependent_paths,

checkpoint test_pretrain:
    """Evaluating test metrics for pretrained models and marking the best one with :best tag"""
    input:
        config="experiments/pretrain/{file}.yaml",
        pretrain_folds=get_test_pretrain_dependent_paths,
    log:
        "logs/test_pretrain/{file}.log",
    output:
        "results/test_pretrain/{file}.SUCCESS",
    threads: 2,
    resources:
        mem_mb=8192,# 8GB
        gpu=1,
        runtime='2h',
    shell:
        "python src/pretrain_test.py --config {input.config} > {log} 2> {log} && touch {output}"


rule finetune:
    input:
        pretrain_status=get_finetune_depend_on_pretrain_status,
        config="experiments/finetune/{file}.yaml",
    params:
        n=config["number_of_folds"],
    output:
        "results/finetune/{file}_fraction-{fraction}.SUCCESS",
    log:
        "logs/finetune/{file}_fraction-{fraction}.log",
    threads: 2,
    resources:
        mem_mb=8192,# 8GB
        gpu=1,

        # 3 hours per 1 fold of 100% data
        runtime=lambda wc: f"{float(wc.fraction) * 3 * config['number_of_folds']}h",
    shell:
        """
        python src/finetune.py --config {input.config} -n {params.n} -f {wildcards.fraction} \
            > {log} 2> {log} \
            && touch {output}
        """


rule finetune_config:
    """Running all data fractions for a given finetune config"""
    input:
        get_finetune_config_dependent_paths,
    output:
        "results/finetune_config/{file}.SUCCESS",
    shell:
        "touch {output}"




# TODO: add rules for statistics, plots, latex report, etc.


apply_defaults()

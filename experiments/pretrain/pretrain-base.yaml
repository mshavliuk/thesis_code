description: &description null # to be overriden
stage: &stage "pretrain"

metric: &metric val_epoch_loss
metric_mode: &metric_mode min

wandb_logger:
  project: Strats # FIXME: Could be fetched from the WANDB_PROJECT environment variable
  job_type: *stage
  notes: *description


module_config:
  model_name: StratsOurs
  loss: MSELoss
  disable_bias_norm_decay: true

  model:
    hid_dim: 64
    num_layers: 2
    num_heads: 8
    dropout: 0.2
    attention_dropout: 0.2
    head_layers:
      - 'forecast_fc'
  optimizer:
    lr: 5.0e-4
    fused: true

common_dataset: &common_dataset
  max_events: 880
  max_minute: 1440 # 24 * 60
  min_input_minutes: 720 # 12*60
  scaler_class: 'VariableStandardScaler'
  prediction_window: 120
  balanced: false
  prediction_gap: 0
  select_top: 128

data_config:
  bootstrap: false
  collator: 'Collator'

  train:
    batch_size: 16
    path: /home/user/.cache/thesis/data/original_strats_data/train
    variables_dropout: 0.2
    repeat_times: 1
    <<: *common_dataset

  val:
    batch_size: 112
    path: /home/user/.cache/thesis/data/original_strats_data/val
    variables_dropout: 0
    repeat_times: 4
    <<: *common_dataset

  test:
    batch_size: 112
    path: /home/user/.cache/thesis/data/original_strats_data/test
    variables_dropout: 0
    repeat_times: 3
    <<: *common_dataset


trainer:
  max_epochs: 160
  precision: bf16-mixed


checkpoint_callback:
  monitor: *metric
  mode: *metric_mode
  save_top_k: 1
  dirpath: /home/user/.cache/thesis/checkpoints/pretrain
#  dirpath: /mnt/ramdisk/checkpoints/pretrain

early_stop_callback:
  monitor: *metric
  mode: *metric_mode
  patience: 10

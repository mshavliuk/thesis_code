from snakemake.io import (
    directory,
    expand,
    temp,
)


configfile: "workflow/config.yaml"

rule all:
    input:
        expand("{data_dir}/raw/{file}.parquet",file=config["mimic_files"], data_dir=config["data_dir"])


rule download_data:
    output:
        "{data_dir}/raw/{file}.csv.gz"
    conda:
        "envs/gloud.yml"
    params:
        google_project_id=config["google_project_id"]
    shell:
        "gsutil -u {params.google_project_id} cp gs://mimiciii-1.4.physionet.org/{wildcards.file}.csv.gz {output}"


rule convert_data_to_parquet:
    conda:
        "envs/pyspark.yml"
    input:
        data="{config['data_dir']}/raw/{file}.csv.gz"
    output:
        directory("{config['data_dir']}/raw/{file}.parquet")
    log:
        stdout="logs/spark_convert_to_parquet_{file}.log",
        stderr="logs/spark_convert_to_parquet_{file}.log"
    shell:
        """
        spark-submit workflow/scripts/csv_gz_to_parquet.py {input.data} {output} > {log.stdout} 2> {log.stderr}
        """
